{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "demo-title",
      "metadata": {},
      "source": [
        "# LangSmith Demo 🦜🛠️\n",
        "\n",
        "LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. What we'll cover...\n",
        "1. **Tracing** - Monitor LLM application runs with detailed observability\n",
        "2. **Playground & Prompts** - Optimize and collaborate on prompts\n",
        "3. **Datasets & Evaluations** - Test applications systematically\n",
        "4. **Annotation Queues** - Enable human feedback and collaboration\n",
        "5. **Automations & Online Evaluations** - Production monitoring\n",
        "6. **Dashboards & Alerts** - Visualize application performance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup-section",
      "metadata": {},
      "source": [
        "## RAG Application\n",
        "\n",
        "![Simple RAG](./images/simple_rag.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16c29d5c",
      "metadata": {},
      "source": [
        "#### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup-code",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tempfile\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders.sitemap import SitemapLoader\n",
        "from langchain_community.vectorstores import SKLearnVectorStore\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langsmith import traceable\n",
        "from openai import OpenAI\n",
        "from typing import List\n",
        "import nest_asyncio\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
        "# os.environ[\"LANGSMITH_TRACING_V2\"] = \"true\"\n",
        "# os.environ[\"LANGCHAIN_PROJECT\"] = \"langsmith-demo\"\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(dotenv_path=\"../../.env\", override=True)\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Configuration\n",
        "MODEL_NAME = \"gpt-4o-mini\"\n",
        "MODEL_PROVIDER = \"openai\"\n",
        "\n",
        "# RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
        "# Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
        "# If you don't know the answer, just say that you don't know. \n",
        "# Use three sentences maximum and keep the answer concise.\n",
        "# \"\"\"\n",
        "\n",
        "# Note that we are pulling our prompt from LangChain's Hub\n",
        "prompt = hub.pull(\"ls-demo-v1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rag-setup",
      "metadata": {},
      "source": [
        "#### Setup VectorDB retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vector-db-setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_vector_db_retriever():\n",
        "    persist_path = os.path.join(tempfile.gettempdir(), \"union.parquet\")\n",
        "    embd = OpenAIEmbeddings()\n",
        "\n",
        "    # If vector store exists, then load it\n",
        "    if os.path.exists(persist_path):\n",
        "        vectorstore = SKLearnVectorStore(\n",
        "            embedding=embd, persist_path=persist_path, serializer=\"parquet\"\n",
        "        )\n",
        "        return vectorstore.as_retriever(lambda_mult=0)\n",
        "\n",
        "    # Otherwise, index LangSmith documents and create new vector store\n",
        "    print(\"Indexing LangSmith documentation...\")\n",
        "    ls_docs_sitemap_loader = SitemapLoader(\n",
        "        web_path=\"https://docs.smith.langchain.com/sitemap.xml\",\n",
        "        continue_on_failure=True,\n",
        "    )\n",
        "    ls_docs = ls_docs_sitemap_loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "        chunk_size=500, chunk_overlap=0\n",
        "    )\n",
        "    doc_splits = text_splitter.split_documents(ls_docs)\n",
        "\n",
        "    vectorstore = SKLearnVectorStore.from_documents(\n",
        "        documents=doc_splits,\n",
        "        embedding=embd,\n",
        "        persist_path=persist_path,\n",
        "        serializer=\"parquet\",\n",
        "    )\n",
        "    vectorstore.persist()\n",
        "    print(f\"Indexed {len(doc_splits)} document chunks\")\n",
        "    return vectorstore.as_retriever(lambda_mult=0)\n",
        "\n",
        "# Initialize retriever\n",
        "retriever = get_vector_db_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "284f6efb",
      "metadata": {},
      "source": [
        "#### Main RAG application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2364fb9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai.types.chat import ChatCompletion, ChatCompletionMessageParam\n",
        "from langsmith.client import convert_prompt_to_openai_format\n",
        "\n",
        "openai_client = OpenAI()\n",
        "\n",
        "@traceable(run_type=\"chain\")\n",
        "def retrieve_documents(question: str):\n",
        "    return retriever.invoke(question)\n",
        "\n",
        "@traceable(\n",
        "    run_type=\"llm\",\n",
        "    metadata={\"ls_provider\": MODEL_PROVIDER, \"ls_model_name\": MODEL_NAME}\n",
        ")\n",
        "def call_openai(messages: List[ChatCompletionMessageParam]) -> ChatCompletion:\n",
        "    return openai_client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=messages,\n",
        "    )\n",
        "\n",
        "@traceable(run_type=\"chain\")\n",
        "def generate_response(question: str, documents):\n",
        "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
        "    formatted_prompt = prompt.invoke({\"context\":formatted_docs, \"question\": question})\n",
        "    messages = convert_prompt_to_openai_format(formatted_prompt)[\"messages\"]\n",
        "    return call_openai(messages)\n",
        "\n",
        "@traceable(run_type=\"chain\")\n",
        "def langsmith_rag(question: str):\n",
        "    documents = retrieve_documents(question)\n",
        "    response = generate_response(question, documents)\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tracing-section",
      "metadata": {},
      "source": [
        "### Tracing: Observability for LLM Applications\n",
        "\n",
        "LangSmith provides comprehensive tracing based on the open-source OpenTelemetry standard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "basic-rag-query",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "langsmith-demo-3\n"
          ]
        }
      ],
      "source": [
        "question = \"What is LangSmith used for?\"\n",
        "answer = langsmith_rag(question)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "playground-section",
      "metadata": {},
      "source": [
        "### Playground & Prompts: Optimize and Collaborate\n",
        "\n",
        "The Playground enables prompt optimization and collaboration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5f48060",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Refresh prompt from LangChain hub\n",
        "prompt = hub.pull(\"ls-demo-v1\")\n",
        "\n",
        "question = \"What is LangSmith used for?\"\n",
        "answer = langsmith_rag(question)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef0ed56c",
      "metadata": {},
      "source": [
        "Let's add some more examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "prompt-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Refresh prompt from LangChain hub\n",
        "prompt = hub.pull(\"ls-demo-v1\")\n",
        "\n",
        "questions = [\n",
        "    \"How do I set up LangSmith tracing?\",\n",
        "    \"What are the key benefits of using LangSmith?\",\n",
        "    \"Can LangSmith work without LangChain?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    answer = langsmith_rag(q)\n",
        "    print(f\"Q: {q}\")\n",
        "    print(f\"A: {answer}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "datasets-section",
      "metadata": {},
      "source": [
        "### Datasets & Evaluations: Systematic Testing\n",
        "\n",
        "Datasets are collections of test data for evaluating your application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "evaluation-setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "example_inputs = [\n",
        "(\"How do I set up tracing to LangSmith if I'm using LangChain?\", \"To set up tracing to LangSmith while using LangChain, you need to set the environment variable `LANGSMITH_TRACING` to 'true'. Additionally, you must set the `LANGSMITH_API_KEY` environment variable to your API key. By default, traces will be logged to a project named \\\"default.\\\"\"),\n",
        "(\"How can I trace with the @traceable decorator?\", \"To trace with the @traceable decorator in Python, simply decorate any function you want to log traces for by adding `@traceable` above the function definition. Ensure that the LANGSMITH_TRACING environment variable is set to 'true' to enable tracing, and also set the LANGSMITH_API_KEY environment variable with your API key. By default, traces will be logged to a project named \\\"default,\\\" but you can configure it to log to a different project if needed.\"),\n",
        "(\"How do I pass metadata in with @traceable?\", \"You can pass metadata with the @traceable decorator by specifying arbitrary key-value pairs as arguments. This allows you to associate additional information, such as the execution environment or user details, with your traces. For more detailed instructions, refer to the LangSmith documentation on adding metadata and tags.\"),\n",
        "(\"What is LangSmith used for in three sentences?\", \"LangSmith is a platform designed for the development, monitoring, and testing of LLM applications. It enables users to collect and analyze unstructured data, debug issues, and create datasets for testing and evaluation. The tool supports various workflows throughout the application development lifecycle, enhancing the overall performance and reliability of LLM applications.\"),\n",
        "(\"What testing capabilities does LangSmith have?\", \"LangSmith offers capabilities for creating datasets of inputs and reference outputs to run tests on LLM applications, supporting a test-driven approach. It allows for bulk uploads of test cases, on-the-fly creation, and exporting from application traces. Additionally, LangSmith facilitates custom evaluations to score test results, enhancing the testing process.\"),\n",
        "(\"Does LangSmith support online evaluation?\", \"Yes, LangSmith supports online evaluation as a feature. It allows you to configure a sample of runs from production to be evaluated, providing feedback on those runs. You can use either custom code or an LLM as a judge for the evaluations.\"),\n",
        "(\"Does LangSmith support offline evaluation?\", \"Yes, LangSmith supports offline evaluation through its evaluation how-to guides and features for managing datasets. Users can manage datasets for offline evaluations and run various types of evaluations, including unit testing and auto-evaluation. This allows for comprehensive testing and improvement of LLM applications.\"),\n",
        "(\"Can LangSmith be used for finetuning and model training?\", \"Yes, LangSmith can be used for fine-tuning and model training. It allows you to capture run traces from your deployment, query and filter this data, and convert it into a format suitable for fine-tuning models. Additionally, you can create training datasets to keep track of the data used for model training.\"),\n",
        "(\"Can LangSmith be used to evaluate agents?\", \"Yes, LangSmith can be used to evaluate agents. It provides various evaluation strategies, including assessing the agent's final response, evaluating individual steps, and analyzing the trajectory of tool calls. These methods help ensure the effectiveness of LLM applications.\"),\n",
        "(\"How do I create user feedback with the LangSmith sdk?\", \"To create user feedback with the LangSmith SDK, you first need to run your application and obtain the `run_id`. Then, you can use the `create_feedback` method, providing the `run_id`, a feedback key, a score, and an optional comment. For example, in Python, it would look like this: `client.create_feedback(run_id, key=\\\"feedback-key\\\", score=1.0, comment=\\\"comment\\\")`.\"),\n",
        "]\n",
        "\n",
        "client = Client()\n",
        "# TODO: Create dataset and fill in dataset id\n",
        "dataset_id = \"\"\n",
        "\n",
        "inputs = [{\"question\": input_prompt} for input_prompt, _ in example_inputs]\n",
        "outputs = [{\"output\": output_answer} for _, output_answer in example_inputs]\n",
        "\n",
        "client.create_examples(\n",
        "  inputs=inputs,\n",
        "  outputs=outputs,\n",
        "  dataset_id=dataset_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "evaluators-demo",
      "metadata": {},
      "source": [
        "### Custom Evaluators\n",
        "\n",
        "LangSmith supports both *LLM-as-Judge* evaluators and custom *code* evaluators. Examples:\n",
        "- Check if an answer is grounded in the provided documents\n",
        "- Score the perceived helpfulness of an answer from 1-10\n",
        "- Validate that an output contains valid Python\n",
        "- For an email assistant, use regex to check that the correct email signature is used"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3252df0b",
      "metadata": {},
      "source": [
        "#### LLM-as-Judge evaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e38b10ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langsmith.evaluation import EvaluationResult\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Similarity_Score(BaseModel):\n",
        "    similarity_score: int = Field(description=\"Similarity score between 1 and 10, where 1 means unrelated and 10 means identical.\")\n",
        "\n",
        "def similarity(inputs: dict, outputs: dict, reference_outputs: dict) -> dict:\n",
        "    prompt = \"\"\"\n",
        "    You are a similarity evaluator comparing the meaning of two response outputs to an input.\n",
        "    The reference_output is the correct answer to the input. The output is what we want to grade to make sure it is similar to the reference_output.\n",
        "    Your task is to assign a score 1-10 based on the following rubric:\n",
        "\n",
        "    <Rubric>\n",
        "        When scoring, you should reward:\n",
        "        - When the output and the reference_output are semantically similar\n",
        "        - When the output and the reference_output are logically similar\n",
        "\n",
        "        When scoring, you should penalize:\n",
        "        - If the output contradicts the reference_output\n",
        "        - If the output is logically inconsistent from the reference_output\n",
        "        - If the output is off topic from the input and reference_output\n",
        "    </Rubric>\n",
        "\n",
        "    <Instructions>\n",
        "        - Carefully read the input, output, and reference_output\n",
        "        - Use the reference_output to determine if output contains errors or logical inconsistencies \n",
        "    </Instructions>\n",
        "\n",
        "    <Reminder>\n",
        "        The reference_output is the correct answer to the input and we want to evaluate the output.\n",
        "    </Reminder>\n",
        "\n",
        "    <input>\n",
        "        {}\n",
        "    </input>\n",
        "\n",
        "    <output>\n",
        "        {}\n",
        "    </output>\n",
        "\n",
        "    Use the reference outputs below to help you evaluate the correctness of the response:\n",
        "    <reference_outputs>\n",
        "        {}\n",
        "    </reference_outputs>\n",
        "    \"\"\".format(inputs[\"question\"], outputs[\"output\"], reference_outputs[\"output\"])\n",
        "    structured_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0).with_structured_output(Similarity_Score)\n",
        "    generation = structured_llm.invoke([HumanMessage(content=prompt)])\n",
        "    return {\"key\": \"similarity\", \"score\": generation.similarity_score}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f686eaca",
      "metadata": {},
      "source": [
        "#### Code Evaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "custom-evaluators",
      "metadata": {},
      "outputs": [],
      "source": [
        "def is_concise_enough(reference_outputs: dict, outputs: dict) -> dict:\n",
        "    score = len(outputs[\"output\"]) < 1.25 * len(reference_outputs[\"output\"])\n",
        "    return {\"key\": \"is_concise\", \"score\": int(score)}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3eb84e90",
      "metadata": {},
      "source": [
        "#### Run Evaluators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a15e7fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langsmith import evaluate, Client\n",
        "\n",
        "client = Client()\n",
        "# TODO add dataset name\n",
        "dataset_name = \"\"\n",
        "\n",
        "def target_function(inputs: dict):\n",
        "    return langsmith_rag(inputs[\"question\"])\n",
        "\n",
        "evaluate( \n",
        "    target_function,\n",
        "    data=dataset_name,\n",
        "    evaluators=[is_concise_enough, similarity],\n",
        "    experiment_prefix=\"ls-demo-1\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "annotation-section",
      "metadata": {},
      "source": [
        "### Annotation Queues: Human Feedback & Collaboration\n",
        "\n",
        "Annotation queues enable developers and subject matter experts to provide structured feedback."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "annotation-examples",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create examples to add to annotation queue\n",
        "annotation_questions = [\n",
        "    \"What are the pricing tiers for LangSmith?\",\n",
        "    \"How does LangSmith compare to other LLM monitoring tools?\",\n",
        "    \"Can I use LangSmith for fine-tuning models?\",\n",
        "    \"What are hosting options for LangSmith?\"\n",
        "]\n",
        "\n",
        "for question in annotation_questions:\n",
        "    answer = langsmith_rag(question)\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "automations-section",
      "metadata": {},
      "source": [
        "### Automations & Online Evaluations: Production Monitoring\n",
        "\n",
        "Automations run on live production interactions to provide continuous monitoring. Examples:\n",
        "- Sample all runs with low frequency and add to annotation queue for human spot checking\n",
        "- Add traces with negative feedback to annotation queue for diagnosis\n",
        "- Add all traces with positive feedback to a golden dataset\n",
        "- For all traces with an error, alert PagerDuty or create Jira ticket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "production-simulation",
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "production_questions = [\n",
        "    \"How do I get started with LangSmith?\",\n",
        "    \"What's the difference between tracing and evaluation?\",\n",
        "    \"How can I monitor my LLM costs?\",\n",
        "    \"What integrations does LangSmith support?\",\n",
        "    \"How do I export my data from LangSmith?\",\n",
        "    \"Can I use custom models with LangSmith?\"\n",
        "]\n",
        "\n",
        "for i in range(3):\n",
        "    question = random.choice(production_questions)\n",
        "    \n",
        "    answer = langsmith_rag(question)\n",
        "        \n",
        "    print(f\"Session {i+1}: {question}\")\n",
        "    print(f\"Answer: {answer}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dashboards-section",
      "metadata": {},
      "source": [
        "### Dashboards: Visualize Performance\n",
        "\n",
        "LangSmith provides both pre-built and custom dashboards for monitoring:\n",
        "\n",
        "**Pre-built Dashboards**:\n",
        "- Request volume and latency\n",
        "- Cost tracking by model\n",
        "- Error rates and types\n",
        "- Token usage patterns\n",
        "\n",
        "**Custom Dashboards**:\n",
        "- Multi-project views\n",
        "- Custom metrics and filters\n",
        "- Business-specific KPIs\n",
        "\n",
        "**Alerts**\n",
        "- Based on errors, feedback, or latency\n",
        "- PagerDuty or Webhook integrations"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ls-demo (3.11.1)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
